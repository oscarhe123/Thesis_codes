{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cd7c360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from DataHandlers import *\n",
    "from nn_models.DBlink_NN import *\n",
    "#from Trainers import *\n",
    "from Utils import *\n",
    "from demo_exp_params import *\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from Trainers_ULM import *\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "tmp_result_dir_exist = os.path.exists(\"./tmp_results\")\n",
    "if not tmp_result_dir_exist:\n",
    "   # Create a tmp_results dir because it does not exist\n",
    "   os.makedirs(\"./tmp_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e0f7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Step I - Parameter Initialization #######\n",
    "# Run flags\n",
    "GenerateTrainData = False\n",
    "GenerateTestData = True\n",
    "TrainNetFlag = False\n",
    "TestOnRealData = False\n",
    "\n",
    "path = r'./' # Path to model\n",
    "model_name = 'LSTM_model' # Model name\n",
    "scale = 4 # Scale factor, the size of the reconstructed image pixels\n",
    "sum_factor = 10 # The temporal window size DBlink uses to sum localizations\n",
    "pixel_size = 160 # Camera pixel size - relevant for experimental data\n",
    "simulated_video_length = 3000 # Length of simulated video - relevant for simulated data generation\n",
    "density = 0.002 # Blinking density (percentage out of the number of non-zero pixels in the simulated structure)\n",
    "img_size = 32 # Simulated image size - relevant for simulated data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca327e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Step II - Training data generation #######\n",
    "trainset_size = 1000 #1024\n",
    "valset_size =  250 #256\n",
    "if(TrainNetFlag):\n",
    "    if(GenerateTrainData):\n",
    "        [X_train, y_train] = Simulate_Train_Data_060622(obs_size=img_size, dataset_size=trainset_size,\n",
    "                                                        video_length=simulated_video_length, emitters_density=density,\n",
    "                                                         scale=scale, sum_range=sum_factor, datatype='tubules')\n",
    "        [X_val, y_val] = Simulate_Train_Data_060622(obs_size=img_size, dataset_size=valset_size,\n",
    "                                                    video_length=simulated_video_length, emitters_density=density,\n",
    "                                                    scale=scale, sum_range=sum_factor, datatype='tubules')\n",
    "        X_train = torch.FloatTensor(X_train)\n",
    "        y_train = torch.FloatTensor(y_train)\n",
    "        X_val = torch.FloatTensor(X_val)\n",
    "        y_val = torch.FloatTensor(y_val)\n",
    "\n",
    "        torch.save(X_train, 'BaseX_train')\n",
    "        torch.save(y_train, 'Basey_train')\n",
    "        torch.save(X_val, 'BaseX_val')\n",
    "        torch.save(y_val, 'Basey_val')\n",
    "    else:\n",
    "        X_train = torch.load('BaseX_train')\n",
    "        y_train = torch.load('Basey_train')\n",
    "        X_val = torch.load('BaseX_val')\n",
    "        y_val = torch.load('Basey_val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d5dbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 300, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "311c7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Step III - Build Model, loss and optimizer #######\n",
    "num_layers = 2 # The number of LSTM layers\n",
    "hidden_channels = 4 # The hidden layer number of channels\n",
    "lr = 1e-4 # Training learning rate\n",
    "window_size = 25 # The number of used windows (in each direction) for the inference of each reconstructed frame\n",
    "betas = (0.99, 0.999) # Parameters of Adam optimizer\n",
    "batch_size = 1\n",
    "epochs = 1\n",
    "patience = 3\n",
    "\n",
    "model = ConvOverlapBLSTM(input_size=(img_size, img_size), input_channels=1, hidden_channels=hidden_channels, num_layers=num_layers, device=device).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=patience, min_lr=1e-9, verbose=True)\n",
    "\n",
    "model_name = 'LSTM_model'\n",
    "\n",
    "####### Step IV - Training the model #######\n",
    "if(TrainNetFlag):\n",
    "    dl_train = CreateDataLoader(X_train, y_train, batch_size=batch_size)\n",
    "    dl_val = CreateDataLoader(X_val, y_val, batch_size=batch_size)\n",
    "\n",
    "    trainer = LSTM_overlap_Trainer(model, criterion, optimizer, scheduler, batch_size, window_size=window_size,\n",
    "                                   vid_length=X_train.shape[1], patience=patience, device=device,modelname= modelname)\n",
    "    trainer.fit(dl_train, dl_val, num_epochs=epochs)\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_name, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe2037d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Generating training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25% 1/4 [00:11<00:33, 11.03s/it]/workspace/code/DBlink/DataHandlers.py:308: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  paths = np.array(paths)\n",
      "100% 4/4 [00:45<00:00, 11.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass through the network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 300/300 [04:16<00:00,  1.17it/s]\n",
      "100% 300/300 [00:00<00:00, 23763.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Completed vid 1\n",
      "Post processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 250/250 [01:20<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Completed vid 1\n",
      "Forward pass through the network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 300/300 [04:13<00:00,  1.18it/s]\n",
      "100% 300/300 [00:00<00:00, 23011.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Completed vid 2\n",
      "Post processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 250/250 [00:58<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Completed vid 2\n",
      "Forward pass through the network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 300/300 [04:13<00:00,  1.18it/s]\n",
      "100% 300/300 [00:00<00:00, 23596.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Completed vid 3\n",
      "Post processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 250/250 [00:48<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Completed vid 3\n",
      "Forward pass through the network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 300/300 [04:13<00:00,  1.18it/s]\n",
      "100% 300/300 [00:00<00:00, 23703.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Completed vid 4\n",
      "Post processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 250/250 [01:14<00:00,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Completed vid 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "####### Step V - Testing the model #######\n",
    "if(TestOnRealData):\n",
    "    analyze_storm_exp_overlap(path_to_model='./{}'.format(model_name),\n",
    "                              exp_class=demo_params(),\n",
    "                              hidden_channels=hidden_channels,\n",
    "                              num_layers=num_layers,\n",
    "                              scale=scale,\n",
    "                              device=device)\n",
    "    post_process_results(r'./tmp_results', 1)\n",
    "else:\n",
    "    testset_size = 4\n",
    "    if(GenerateTestData):\n",
    "        [X_test, y_test] = Simulate_Train_Data_060622(obs_size=img_size, dataset_size=testset_size,\n",
    "                                                      video_length=simulated_video_length, emitters_density=density,\n",
    "                                                      scale=scale, sum_range=sum_factor, datatype='tubules')\n",
    "        X_test = torch.FloatTensor(X_test)\n",
    "        y_test = torch.FloatTensor(y_test)\n",
    "        torch.save(X_test, 'BaseX_test')\n",
    "        torch.save(y_test, 'Basey_test')\n",
    "    else:\n",
    "        X_test = torch.load('X_test')\n",
    "        y_test = torch.load('y_test')\n",
    "\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "\n",
    "    N, T, C, H, W = X_test.shape\n",
    "\n",
    "    model = ConvOverlapBLSTM(input_size=(img_size, img_size), input_channels=1, hidden_channels=hidden_channels,\n",
    "                                num_layers=num_layers, device=device).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(path, model_name), map_location=torch.device(device)))\n",
    "\n",
    "    down = torch.zeros(X_test.size(1), requires_grad=False, dtype=torch.int)\n",
    "    up = torch.zeros(X_test.size(1), requires_grad=False, dtype=torch.int)\n",
    "    out_ind = torch.zeros(X_test.size(1), requires_grad=False, dtype=torch.int)\n",
    "    for i in range(X_test.size(1)):\n",
    "        down[i] = torch.max(torch.IntTensor([0, i - sum_factor * window_size]))\n",
    "        up[i] = torch.min(torch.IntTensor([X_test.size(1), i + sum_factor * window_size]))\n",
    "        out_ind[i] = i - down[i]\n",
    "        \n",
    "    sum_factor = 1 # sum_factor of the generated data is already summed !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "    for i in range(X_test.size(0)):\n",
    "        out = []\n",
    "        print('Forward pass through the network')\n",
    "        with torch.no_grad():\n",
    "            for j in tqdm(range(X_test.shape[1])):\n",
    "                curr_out = model(X_test[i:i + 1, down[j]:up[j]:sum_factor],\n",
    "                                 torch.flip(X_test[i:i + 1, down[j]:up[j]:sum_factor], dims=[1]))\n",
    "                curr_out = curr_out.detach().cpu()[0, int(out_ind[j] / sum_factor)]\n",
    "                out.append(curr_out)\n",
    "\n",
    "        out = torch.stack(out, dim=1)\n",
    "\n",
    "        curr_vid = np.zeros([1, X_test.size(1), C, H, W])\n",
    "        for j in tqdm(range(X_test.size(1))):\n",
    "            curr_vid[0, j] = 255 * normalize_input_01(out[0, j].numpy())\n",
    "\n",
    "        np.save('tmp_results/np_vid_{}'.format(i + 1), curr_vid[0, :-2*window_size])\n",
    "        np.save('tmp_results/gt_vid_{}'.format(i + 1), y_test[i, :-2*window_size].detach().cpu().numpy())\n",
    "\n",
    "        print(\"-I- Completed vid\", i + 1)\n",
    "\n",
    "        # Post process reconstruction and generate output video\n",
    "        post_process_results(r'./tmp_results', i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb7f1be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 300, 1, 128, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29403b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
