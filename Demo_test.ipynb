{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14f498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from DataHandlers import *\n",
    "from nn_models.DBlink_NN import *\n",
    "from Trainers import *\n",
    "from Utils import *\n",
    "from demo_exp_params import *\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "tmp_result_dir_exist = os.path.exists(\"./tmp_results\")\n",
    "if not tmp_result_dir_exist:\n",
    "   # Create a tmp_results dir because it does not exist\n",
    "   os.makedirs(\"./tmp_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba55f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Step I - Parameter Initialization #######\n",
    "# Run flags\n",
    "GenerateTrainData = False\n",
    "GenerateTestData = True\n",
    "TrainNetFlag = False\n",
    "TestOnRealData = False\n",
    "\n",
    "path = r'./' # Path to model\n",
    "model_name = 'LSTM_model' # Model name\n",
    "scale = 1 # Scale factor, the size of the reconstructed image pixels\n",
    "sum_factor = 10 # The temporal window size DBlink uses to sum localizations\n",
    "pixel_size = 160 # Camera pixel size - relevant for experimental data\n",
    "simulated_video_length = 3000 # Length of simulated video - relevant for simulated data generation\n",
    "density = 0.002 # Blinking density (percentage out of the number of non-zero pixels in the simulated structure)\n",
    "img_size = 32 # Simulated image size - relevant for simulated data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d89bbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Step II - Training data generation #######\n",
    "trainset_size = 1000 #1024\n",
    "valset_size =  250 #256\n",
    "if(TrainNetFlag):\n",
    "    if(GenerateTrainData):\n",
    "        [X_train, y_train] = Simulate_Train_Data_060622(obs_size=img_size, dataset_size=trainset_size,\n",
    "                                                        video_length=simulated_video_length, emitters_density=density,\n",
    "                                                         scale=scale, sum_range=sum_factor, datatype='tubules')\n",
    "        [X_val, y_val] = Simulate_Train_Data_060622(obs_size=img_size, dataset_size=valset_size,\n",
    "                                                    video_length=simulated_video_length, emitters_density=density,\n",
    "                                                    scale=scale, sum_range=sum_factor, datatype='tubules')\n",
    "        X_train = torch.FloatTensor(X_train)\n",
    "        y_train = torch.FloatTensor(y_train)\n",
    "        X_val = torch.FloatTensor(X_val)\n",
    "        y_val = torch.FloatTensor(y_val)\n",
    "\n",
    "        torch.save(X_train, 'BaseX_train')\n",
    "        torch.save(y_train, 'Basey_train')\n",
    "        torch.save(X_val, 'BaseX_val')\n",
    "        torch.save(y_val, 'Basey_val')\n",
    "    else:\n",
    "        X_train = torch.load('BaseX_train')\n",
    "        y_train = torch.load('Basey_train')\n",
    "        X_val = torch.load('BaseX_val')\n",
    "        y_val = torch.load('Basey_val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5806c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 300, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a77ff0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EPOCH 1/150 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 1, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 1/1000 [01:01<17:06:13, 61.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 1, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 1/1000 [01:25<23:47:11, 85.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     dl_val \u001b[38;5;241m=\u001b[39m CreateDataLoader(X_val, y_val, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m     21\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m LSTM_overlap_Trainer(model, criterion, optimizer, scheduler, batch_size, window_size\u001b[38;5;241m=\u001b[39mwindow_size,\n\u001b[1;32m     22\u001b[0m                                    vid_length\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], patience\u001b[38;5;241m=\u001b[39mpatience, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), model_name)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/workspace/code/DBlink/Trainers.py:134\u001b[0m, in \u001b[0;36mLSTM_overlap_Trainer.fit\u001b[0;34m(self, dl_train, dl_test, num_epochs, early_stopping, print_every, **kw)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--- EPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, num_epochs))\n\u001b[0;32m--> 134\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m    137\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_epoch(dl_test, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m/workspace/code/DBlink/Trainers.py:181\u001b[0m, in \u001b[0;36mLSTM_overlap_Trainer.train_epoch\u001b[0;34m(self, dl_train, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 181\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# print(out.shape)                       \u001b[39;00m\n\u001b[1;32m    184\u001b[0m                           \n\u001b[1;32m    185\u001b[0m                      \u001b[38;5;66;03m# out_ind=self.out_ind[j])\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# Compute Loss\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m#consistency_loss = torch.sum(torch.abs(out - prev_out.detach()))  # consistency between only 2 frames (prevout and out )<-- only out_ind frames\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     consistency_loss \u001b[38;5;241m=\u001b[39m consistency_reg(out)  \u001b[38;5;66;03m#average consistency loss between all provided frames \u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/code/DBlink/nn_models/NN_model.py:66\u001b[0m, in \u001b[0;36mConvOverlapBLSTM.forward\u001b[0;34m(self, xforward, xreverse)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, xforward, xreverse):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    xforward, xreverse = B T C H W tensors.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     y_out_fwd, hidden_fwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxforward\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     y_out_rev, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreverse_net(xreverse)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Take only the output of the last layer\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/code/DBlink/nn_models/NN_model.py:431\u001b[0m, in \u001b[0;36mConvLSTM.forward\u001b[0;34m(self, input_tensor, hidden_state, prop_hidden)\u001b[0m\n\u001b[1;32m    428\u001b[0m output_inner \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len):\n\u001b[0;32m--> 431\u001b[0m     h, c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_layer_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcur_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m     output_inner\u001b[38;5;241m.\u001b[39mappend(h)\n\u001b[1;32m    435\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(output_inner, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/code/DBlink/nn_models/NN_model.py:346\u001b[0m, in \u001b[0;36mConvLSTMCell.forward\u001b[0;34m(self, input_tensor, cur_state)\u001b[0m\n\u001b[1;32m    344\u001b[0m cc_i, cc_f, cc_o, cc_g \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(combined_conv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    345\u001b[0m i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(cc_i)\n\u001b[0;32m--> 346\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcc_f\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m o \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(cc_o)\n\u001b[1;32m    348\u001b[0m g \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(cc_g)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####### Step III - Build Model, loss and optimizer #######\n",
    "num_layers = 2 # The number of LSTM layers\n",
    "hidden_channels = 4 # The hidden layer number of channels\n",
    "lr = 1e-4 # Training learning rate\n",
    "window_size = 25 # The number of used windows (in each direction) for the inference of each reconstructed frame\n",
    "betas = (0.99, 0.999) # Parameters of Adam optimizer\n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "patience = 3\n",
    "\n",
    "model = ConvOverlapBLSTM(input_size=(img_size, img_size), input_channels=1, hidden_channels=hidden_channels, num_layers=num_layers, device=device).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=patience, min_lr=1e-9, verbose=True)\n",
    "\n",
    "####### Step IV - Training the model #######\n",
    "if(TrainNetFlag):\n",
    "    dl_train = CreateDataLoader(X_train, y_train, batch_size=batch_size)\n",
    "    dl_val = CreateDataLoader(X_val, y_val, batch_size=batch_size)\n",
    "\n",
    "    trainer = LSTM_overlap_Trainer(model, criterion, optimizer, scheduler, batch_size, window_size=window_size,\n",
    "                                   vid_length=X_train.shape[1], patience=patience, device=device)\n",
    "    trainer.fit(dl_train, dl_val, num_epochs=epochs)\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_name, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b7334b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Generating training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/4 [00:00<?, ?it/s]/workspace/code/DBlink/DataHandlers.py:308: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  paths = np.array(paths)\n",
      "100% 4/4 [00:45<00:00, 11.37s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hidden_channels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m y_test \u001b[38;5;241m=\u001b[39m y_test\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     27\u001b[0m N, T, C, H, W \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 29\u001b[0m model \u001b[38;5;241m=\u001b[39m ConvOverlapBLSTM(input_size\u001b[38;5;241m=\u001b[39m(img_size, img_size), input_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, hidden_channels\u001b[38;5;241m=\u001b[39m\u001b[43mhidden_channels\u001b[49m,\n\u001b[1;32m     30\u001b[0m                             num_layers\u001b[38;5;241m=\u001b[39mnum_layers, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, model_name), map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(device)))\n\u001b[1;32m     33\u001b[0m down \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(X_test\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hidden_channels' is not defined"
     ]
    }
   ],
   "source": [
    "####### Step V - Testing the model #######\n",
    "if(TestOnRealData):\n",
    "    analyze_storm_exp_overlap(path_to_model='./{}'.format(model_name),\n",
    "                              exp_class=demo_params(),\n",
    "                              hidden_channels=hidden_channels,\n",
    "                              num_layers=num_layers,\n",
    "                              scale=scale,\n",
    "                              device=device)\n",
    "    post_process_results(r'./tmp_results', 1)\n",
    "else:\n",
    "    testset_size = 4\n",
    "    if(GenerateTestData):\n",
    "        [X_test, y_test] = Simulate_Train_Data_060622(obs_size=img_size, dataset_size=testset_size,\n",
    "                                                      video_length=simulated_video_length, emitters_density=density,\n",
    "                                                      scale=scale, sum_range=sum_factor, datatype='tubules')\n",
    "        X_test = torch.FloatTensor(X_test)\n",
    "        y_test = torch.FloatTensor(y_test)\n",
    "        torch.save(X_test, 'BaseX_test')\n",
    "        torch.save(y_test, 'Basey_test')\n",
    "    else:\n",
    "        X_test = torch.load('X_test')\n",
    "        y_test = torch.load('y_test')\n",
    "\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "\n",
    "    N, T, C, H, W = X_test.shape\n",
    "\n",
    "    model = ConvOverlapBLSTM(input_size=(img_size, img_size), input_channels=1, hidden_channels=hidden_channels,\n",
    "                                num_layers=num_layers, device=device).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(path, model_name), map_location=torch.device(device)))\n",
    "\n",
    "    down = torch.zeros(X_test.size(1), requires_grad=False, dtype=torch.int)\n",
    "    up = torch.zeros(X_test.size(1), requires_grad=False, dtype=torch.int)\n",
    "    out_ind = torch.zeros(X_test.size(1), requires_grad=False, dtype=torch.int)\n",
    "    for i in range(X_test.size(1)):\n",
    "        down[i] = torch.max(torch.IntTensor([0, i - sum_factor * window_size]))\n",
    "        up[i] = torch.min(torch.IntTensor([X_test.size(1), i + sum_factor * window_size]))\n",
    "        out_ind[i] = i - down[i]\n",
    "        \n",
    "    sum_factor = 1 # sum_factor of the generated data is already summed !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "    for i in range(X_test.size(0)):\n",
    "        out = []\n",
    "        print('Forward pass through the network')\n",
    "        with torch.no_grad():\n",
    "            for j in tqdm(range(X_test.shape[1])):\n",
    "                curr_out = model(X_test[i:i + 1, down[j]:up[j]:sum_factor],\n",
    "                                 torch.flip(X_test[i:i + 1, down[j]:up[j]:sum_factor], dims=[1]))\n",
    "                curr_out = curr_out.detach().cpu()[0, int(out_ind[j] / sum_factor)]\n",
    "                out.append(curr_out)\n",
    "\n",
    "        out = torch.stack(out, dim=1)\n",
    "\n",
    "        curr_vid = np.zeros([1, X_test.size(1), C, H, W])\n",
    "        for j in tqdm(range(X_test.size(1))):\n",
    "            curr_vid[0, j] = 255 * normalize_input_01(out[0, j].numpy())\n",
    "\n",
    "        np.save('tmp_results/np_vid_{}'.format(i + 1), curr_vid[0, :-2*window_size])\n",
    "        np.save('tmp_results/gt_vid_{}'.format(i + 1), y_test[i, :-2*window_size].detach().cpu().numpy())\n",
    "\n",
    "        print(\"-I- Completed vid\", i + 1)\n",
    "\n",
    "        # Post process reconstruction and generate output video\n",
    "        post_process_results(r'./tmp_results', i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a4bdfff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 300, 1, 128, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f29a67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
